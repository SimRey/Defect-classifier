{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steel Plate Defect Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosters\n",
    "\n",
    "In this notebook different boosters will be explored, as well as the optimization of its hyperparameters. All the boosting methods will be compared and ensambled in a voting clasifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pipeline2 import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/train.csv\")\n",
    "pl = Pipeline(df_train, True)\n",
    "df_train, le, sc, faul_encoder = pl.run()\n",
    "\n",
    "df_test = pd.read_csv(\"Data/test.csv\")\n",
    "pl = Pipeline(df_test, False)\n",
    "df_test = pl.run(le, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. XGBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['faults'])\n",
    "y = df_train['faults']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 8,\n",
    "    'tree_method': 'hist',\n",
    "    'grow_policy': 'depthwise',\n",
    "    'boosting_type': 'gbtree',\n",
    "    'enable_categorical': True,\n",
    "\n",
    "    # Variables\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-3), np.log(0.5)),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 2000, 10),\n",
    "    'gamma': hp.uniform('gamma', 0.5, 1),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 2, 100),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1), \n",
    "    \n",
    "    'verbosity': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:04<00:00, 21.21s/trial, best loss: -0.8606209695747803]\n",
      "Best parameters: {'colsample_bytree': 0.7249204152698141, 'gamma': 0.8093507699039332, 'learning_rate': 0.07512032619541904, 'max_depth': 5.0, 'min_child_weight': 41.14804068289674, 'n_estimators': 190.0, 'subsample': 0.5587606280948411}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['learning_rate'] = float(params['learning_rate'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['gamma'] = float(params['gamma'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    params['colsample_bytree'] = float(params['colsample_bytree'])\n",
    "    params['subsample'] = float(params['subsample'])\n",
    "    \n",
    "    # Create XGBoost classifier with given parameters\n",
    "    clf = XGBClassifier(**params)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    score = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=20)\n",
    "print(\"Best parameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8497293044972776\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 8,\n",
    "    'tree_method': 'hist',\n",
    "    'grow_policy': 'depthwise',\n",
    "    'boosting_type': 'gbtree',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.07512032619541904,\n",
    "    'n_estimators': 190,\n",
    "    'gamma': 0.8093507699039332,\n",
    "    'min_child_weight': 41.14804068289674,\n",
    "    'colsample_bytree': 0.7249204152698141,\n",
    "    'subsample': 0.5587606280948411, \n",
    "    \n",
    "    'verbosity': 0}\n",
    "\n",
    "\n",
    "xgb_clf = XGBClassifier(**hyperparams)\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_prob = xgb_clf.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the model were obtained from the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8540490773954753\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 8,\n",
    "    'n_estimators': 918,\n",
    "    'learning_rate': 0.0014,\n",
    "    'max_depth': 5,\n",
    "    'reg_alpha': 0.9522134628349151,\n",
    "    'reg_lambda': 0.07875944420059292,\n",
    "    'num_leaves': 20,\n",
    "    'subsample': 0.33327260735952596,\n",
    "    'colsample_bytree': 0.45916663480321157,   \n",
    "    'verbosity': -1}\n",
    "\n",
    "lgbm_clf = LGBMClassifier(**hyperparams)\n",
    "\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_prob = lgbm_clf.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "print(score)                          \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8541584520303275\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('XGB', xgb_clf),('LGBM', lgbm_clf)],\n",
    "    voting='soft',n_jobs=-1)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_prob = voting_clf.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "print(score)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The voting classifier guarantees a score value that is equal to or greater than the score of the best-performing model within the voting system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
